{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6qfaWcyv__9s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6qfaWcyv__9s",
    "outputId": "748ee831-0925-4e23-e265-77339f959bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.40.0)\n",
      "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.5.0)\n",
      "Collecting tflearn\n",
      "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 5.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.19.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
      "Building wheels for collected packages: tflearn\n",
      "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=473ae5a1618c7e5e88c5ed740c6255be63a7ec9b51ac2e57a0437e4d1476e26a\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/14/2e/1d8e28cc47a5a931a2fb82438c9e37ef9246cc6a3774520271\n",
      "Successfully built tflearn\n",
      "Installing collected packages: tflearn\n",
      "Successfully installed tflearn-0.5.0\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.5.0-py2.py3-none-any.whl (254 kB)\n",
      "\u001b[K     |████████████████████████████████| 254 kB 5.0 MB/s \n",
      "\u001b[?25hCollecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.2.6)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting pyOpenSSL>=16.2.0\n",
      "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 3.5 MB/s \n",
      "\u001b[?25hCollecting Twisted[http2]>=17.9.0\n",
      "  Downloading Twisted-21.7.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 6.1 MB/s \n",
      "\u001b[?25hCollecting zope.interface>=4.1.3\n",
      "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
      "\u001b[K     |████████████████████████████████| 251 kB 58.0 MB/s \n",
      "\u001b[?25hCollecting protego>=0.1.15\n",
      "  Downloading Protego-0.1.16.tar.gz (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 52.8 MB/s \n",
      "\u001b[?25hCollecting h2<4.0,>=3.0\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 4.0 MB/s \n",
      "\u001b[?25hCollecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.zip (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 5.3 MB/s \n",
      "\u001b[?25hCollecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting cryptography>=2.0\n",
      "  Downloading cryptography-35.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 52.4 MB/s \n",
      "\u001b[?25hCollecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.4.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->scrapy) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.20)\n",
      "Collecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (21.2.0)\n",
      "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted[http2]>=17.9.0->scrapy) (3.7.4.3)\n",
      "Collecting Automat>=0.8.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting incremental>=21.3.0\n",
      "  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 3.5 MB/s \n",
      "\u001b[?25hCollecting priority<2.0,>=1.1.0\n",
      "  Downloading priority-1.3.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted[http2]>=17.9.0->scrapy) (2.10)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface>=4.1.3->scrapy) (57.4.0)\n",
      "Building wheels for collected packages: protego, PyDispatcher\n",
      "  Building wheel for protego (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=7782 sha256=85722c6efb3ddb1e25a033c8e885123433bbeed192398373b7423d04e02eef76\n",
      "  Stored in directory: /root/.cache/pip/wheels/ca/44/01/3592ccfbcfaee4ab297c4097e6e9dbe1c7697e3531a39877ab\n",
      "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11515 sha256=80cfdeb8414361e41b71e5781728972cf6444a84fb5f31fef9c69f82ad663533\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/18/21/3c6a732eaa69a339198e08bb63b7da2c45933a3428b29ec454\n",
      "Successfully built protego PyDispatcher\n",
      "Installing collected packages: zope.interface, w3lib, incremental, hyperlink, hyperframe, hpack, cssselect, constantly, Automat, Twisted, priority, parsel, jmespath, itemadapter, h2, cryptography, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Twisted-21.7.0 constantly-15.1.0 cryptography-35.0.0 cssselect-1.1.0 h2-3.2.0 hpack-3.0.0 hyperframe-5.2.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.4.0 itemloaders-1.0.4 jmespath-0.10.0 parsel-1.6.0 priority-1.3.0 protego-0.1.16 pyOpenSSL-21.0.0 queuelib-1.6.2 scrapy-2.5.0 service-identity-21.1.0 w3lib-1.22.0 zope.interface-5.4.0\n"
     ]
    }
   ],
   "source": [
    "# Instalando módulos novos\n",
    "! pip install numpy\n",
    "! pip install sklearn\n",
    "! pip install tensorflow\n",
    "! pip install tflearn\n",
    "! pip install nltk\n",
    "#! pip install pypiwin32\n",
    "! pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "OiWMWORF__93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiWMWORF__93",
    "outputId": "ec6d5459-69d5-40ff-e8c4-3b6ad9723880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Importações Iniciais\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import codecs\n",
    "import numpy as np\n",
    "import tensorflow \n",
    "import tflearn\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "phhzy5OjH9UP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "phhzy5OjH9UP",
    "outputId": "3cea3070-321e-47f4-96ba-467681097c91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tflearn in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "j-4l8LSRHzE0",
   "metadata": {
    "id": "j-4l8LSRHzE0"
   },
   "outputs": [],
   "source": [
    "import tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "g6s66udEH0Em",
   "metadata": {
    "id": "g6s66udEH0Em"
   },
   "outputs": [],
   "source": [
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "QWrmpZIk__97",
   "metadata": {
    "id": "QWrmpZIk__97"
   },
   "outputs": [],
   "source": [
    "# definição de variaveis \n",
    "FILES_DIR = 'teste'\n",
    "MINIMO_LETRAS = 3\n",
    "ACENTOS = ['@', '.', '!','?',',','$','-','\\'s','g','(',')','[',']',':','\\n']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "NJnhu5aqiLHz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJnhu5aqiLHz",
    "outputId": "fea491c0-0649-48d1-ae2b-90307ee6ca34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "i8BvT0Fs__98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8BvT0Fs__98",
    "outputId": "a3ad617d-d85d-4dea-9361-3bde2fd94730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Existem 3 textos. \n",
      "Processamento em 0.8376777172088623s\n"
     ]
    }
   ],
   "source": [
    "# Lendo os arquivos\n",
    "start = time.time()\n",
    "documentos = []\n",
    "resultados = []\n",
    "i = 0\n",
    "for _, _, files in os.walk(os.path.join('/content/drive/MyDrive/Colab_Notebooks/FILES_DIR')):\n",
    "    for f in files:\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print('{} arquivos lidos. lendo {}'.format(i,f))\n",
    "        with codecs.open(os.path.join('/content/drive/MyDrive/Colab_Notebooks/FILES_DIR', f),'r',encoding=\"utf8\", errors='ignore') as f:                     \n",
    "            documentos.append(f.read())\n",
    "        resultados.append(f)\n",
    "end = time.time()                \n",
    "print ('\\nExistem {} textos. \\nProcessamento em {}s'.format(len(documentos),end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8vCFRWcyxWNr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vCFRWcyxWNr",
    "outputId": "d1eadef5-7196-4173-ba46-ba5ccf13203b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "joWlFGJ_xYVE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "joWlFGJ_xYVE",
    "outputId": "fd5ac2cb-7832-417a-970a-a0aa433ea167"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('all')\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "S85qUQQo__9_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S85qUQQo__9_",
    "outputId": "24b9fa35-6a96-4f15-8e32-fde70777c716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 documentos tokenizados. \n",
      "Processamento em 0.8376777172088623s\n"
     ]
    }
   ],
   "source": [
    "# Removendo as tags HTML e Tokenizando o documento\n",
    "cleanr = re.compile('<.*?>')\n",
    "documentos_tokenizados = []\n",
    "for documento in documentos:\n",
    "    cleantext = re.sub(cleanr, '', documento)\n",
    "    documentos_tokenizados.append(word_tokenize(cleantext))\n",
    "print('{} documentos tokenizados. \\nProcessamento em {}s'.format(len(documentos_tokenizados),end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "porUkj7C__-A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "porUkj7C__-A",
    "outputId": "5ccc7232-562e-49d2-e82e-a75d23864733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'casa', 'da', 'sogra', 'no', 'aqui', 'ELIO', 'GASPARINI', 'Opresidente', 'George', 'Bush', 'est', 'envenenando', 'a', 'relao', 'dos', 'Estados', 'Unidos', 'com', 'o', 'mundo', '.', 'Seu', 'governo', 'decidiu', 'que', 'a', 'partir', 'de', 'amanh', 'os', 'estrangeiros', 'que', 'entrarem', 'em', 'territrio', 'americano', 'sero', 'fotografados', 'e', 'deixaro', 'as', 'impresses', 'digitais', 'num', 'banco', 'de', 'dados', '.', '(', 'O', 'procedimento', 'ser', 'feito', 'sem', 'sujar', 'a', 'mo', 'das', 'pessoas', '.', ')', 'Ficaro', 'fora', 'da', 'regra', 'os', 'nacionais', 'de', '27', 'pases', ',', 'quase', 'todos', 'europeus', '.', 'provvel', 'que', 'a', 'medida', 'atinja', '25', 'milhes', 'de', 'pessoas', '.', 'A', 'nao', 'brasileira', 'j', 'passou', 'pelo', 'constrangimento', 'de', 'saber', ',', 'em', '2001', ',', 'que', 'seu', 'ministro', 'das', 'Relaes', 'Exteriores', ',', 'Celso', 'Lafer', ',', 'teve', 'que', 'tirar', 'os', 'sapatos', 'em', 'duas', 'revistas', 'diferentes', 'durante', 'suas', 'viagens', 'aos', 'Estados', 'Unidos', '.', '(', 'No', 'foi', 'nico', ',', 'a', 'impertinncia', 'atingiu', 'o', 'russo', 'e', 'a', 'chilena', '.', ')', 'Bem-aventurado', 'o', 'juiz', 'federal', 'Julier', 'Sebastio', 'da', 'Silva', ',', 'que', 'ordenou', 'Polcia', 'a', 'imediata', 'reciprocidade', ':', 'na', 'quinta-feira', 'os', 'americanos', 'comearam', 'a', 'ser', 'fotografados', 'e', 'identificados', 'pelas', 'digitais', 'ao', 'entrar', 'em', 'territrio', 'brasileiro', '.', 'Yes', ',', 'ns', 'temos', 'juzes', '.', 'possvel', 'que', 'a', 'ordem', 'de', 'Silva', 'seja', 'contestada', ',', 'mas', 'necessrio', 'que', 'o', 'governo', 'brasileiro', 'd', 'alguma', 'resposta', 'ao', 'tratamento', 'que', 'seus', 'nacionais', 'vm', 'recebendo', 'da', 'burocracia', 'americana', '.', 'Lula', 'repete', ',', 'com', 'razo', ',', 'que', 'o', 'governo', 'deve', 'defender', 'os', 'interesses', 'dos', 'brasileiros', '.', 'Est', 'diante', 'de', 'um', 'belo', 'caso', '.', 'Admita-se', 'que', 'a', 'deciso', 'do', 'juiz', 'seja', 'uma', 'demasia', '.', 'Ele', 'o', 'diretor', 'jurdico', 'da', 'Associao', 'dos', 'Juzes', 'Federais', '.', 'Sua', 'comparao', 'das', 'leis', 'de', 'Bush', 'com', 'os', 'piores', 'horrores', 'do', 'nazismo', 'um', 'horror', 'de', 'exagero', 'histrico', '.', 'Mesmo', 'assim', ',', 'h', 'campo', 'para', 'a', 'reciprocidade', '.', 'Depois', 'de', 'colecionar', 'abraos', 'de', 'ditadores', 'como', 'o', 'eterno', 'Fidel', 'Castro', ',', 'o', 'jovem', 'Assad', 'e', 'o', 'velho', 'Kadafi', ',', 'Lula', 'tem', 'a', 'oportunidade', 'de', 'praticar', 'um', 'ato', 'diplomtico', 'que', 'tem', 'a', 'ver', 'com', 'com', 'a', 'vida', 'dos', 'cidados', 'que', 'governa', '.', 'O', 'governo', 'americano', 'exige', 'que', 'os', 'brasileiros', 'que', 'pretendem', 'ir', 'aos', 'Estados', 'Unidos', 'compaream', 'aos', 'seus', 'consulados', 'para', 'entrevistas', 'pessoais', '.', 'Um', 'cidado', 'que', 'vive', 'no', 'Mato', 'Grosso', 'do', 'Sul', 'deve', 'vir', 'a', 'So', 'Paulo', 'para', 'cumprir', 'essa', 'exigncia', '.', 'inteiro', 'direito', 'dos', 'americanos', 'pedir', 'o', 'que', 'bem', 'entenderem', ',', 'pois', 'o', 'pas', 'deles', 'e', 's', 'vai', 'para', 'l', 'quem', 'quer', '.', 'direito', 'dos', 'brasileiros', 'pedir', 'aos', 'americanos', 'que', 'cumpram', 'por', 'c', 'o', 'que', 'exigem', 'por', 'l.', 'Ocorreram', 'casos', 'grotescos', 'de', 'humilhao', 'de', 'brasileiros', 'que', 'entravam', ',', 'com', 'a', 'devida', 'documentao', ',', 'em', 'territrio', 'americano', '.', 'H', 'poucos', 'meses', 'uma', 'mdica', ',', 'ex-presidente', 'da', 'Sociedade', 'Mineira', 'de', 'Cardiologia', ',', 'foi', 'deportada', 'quando', 'viajava', 'para', 'uma', 'reunio', 'internacional', '.', 'Tecnicamente', ',', 'tinha', 'o', 'visto', 'errado', ',', 'pois', 'o', 'carimbo', 'de', 'turista', 'pode', 'ser', 'considerado', 'imprprio', 'para', 'quem', 'vai', 'a', 'um', 'congresso', '.', 'A', 'bem-aventurana', 'posta', 'na', 'mesa', 'pelo', 'juiz', 'Julier', 'Sebastio', 'da', 'Silva', 'deve', 'ser', 'mantida', ',', 'ou', 'refinada', '.', 'Coisa', 'assim', ':', 'todos', 'os', 'americanos', 'que', 'pedirem', 'visto', 'para', 'entrar', 'no', 'Brasil', 'recebem', 'um', 'folheto', 'com', 'a', 'descrio', 'das', 'exigncias', 'que', 'o', 'seu', 'governo', 'faz', 'para', 'a', 'entrada', 'de', 'brasileiros', '.', 'Seriam', 'convidados', 'a', 'assinar', 'um', 'documento', ',', 'pedindo', 'ao', 'governo', 'de', 'Pindorama', 'que', 'no', 'os', 'trate', 'como', 'o', 'governo', 'americano', 'trata', 'os', 'brasileiros', '.', 'Quem', 'pedir', ',', 'leva', '.', 'Quem', 'preferir', 'a', 'reciprocidade', ',', 'ganha', 'o', 'Tratamento', 'George', 'W.', 'Bush', '.', 'O', 'que', 'no', 'se', 'pode', 'deixar', 'o', 'doutor', 'Bush', 'acreditar', 'que', 'isso', 'aqui', 'a', 'casa', 'da', 'sogra', '.', 'Data', 'da', 'Publicao', ':', '01/01/2004', '-', 'Pagina', ':', 'Editoria', ':', 'At2']\n"
     ]
    }
   ],
   "source": [
    "print(documentos_tokenizados[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7S3bCQmq__-C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7S3bCQmq__-C",
    "outputId": "9aadf18f-0f1f-42f4-8b2e-6d143302328a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 documentos removidos as stop words e palavras ignoradas. \n",
      "Processamento em 0.0037174224853515625s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ptbr_stops = set(stopwords.words('portuguese'))\n",
    "documentos_sem_stop_words_e_ignore = []\n",
    "for documento in documentos_tokenizados:    \n",
    "    aux = [w for w in documento if w not in ptbr_stops]   \n",
    "    aux2 = [w for w in aux if w not in ACENTOS]\n",
    "    documentos_sem_stop_words_e_ignore.append([w for w in aux2 if len(w) >= MINIMO_LETRAS])\n",
    "end = time.time()\n",
    "print ('{} documentos removidos as stop words e palavras ignoradas. \\nProcessamento em {}s'\n",
    "       .format(len(documentos_sem_stop_words_e_ignore),end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7pa7Dkt0__-E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7pa7Dkt0__-E",
    "outputId": "e5abf8e8-339b-43bb-c4c3-8496e3395e2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['casa', 'sogra', 'aqui', 'ELIO', 'GASPARINI', 'Opresidente', 'George', 'Bush', 'est', 'envenenando', 'relao', 'Estados', 'Unidos', 'mundo', 'Seu', 'governo', 'decidiu', 'partir', 'amanh', 'estrangeiros', 'entrarem', 'territrio', 'americano', 'sero', 'fotografados', 'deixaro', 'impresses', 'digitais', 'banco', 'dados', 'procedimento', 'ser', 'feito', 'sujar', 'pessoas', 'Ficaro', 'regra', 'nacionais', 'pases', 'quase', 'todos', 'europeus', 'provvel', 'medida', 'atinja', 'milhes', 'pessoas', 'nao', 'brasileira', 'passou', 'constrangimento', 'saber', '2001', 'ministro', 'Relaes', 'Exteriores', 'Celso', 'Lafer', 'tirar', 'sapatos', 'duas', 'revistas', 'diferentes', 'durante', 'viagens', 'Estados', 'Unidos', 'nico', 'impertinncia', 'atingiu', 'russo', 'chilena', 'Bem-aventurado', 'juiz', 'federal', 'Julier', 'Sebastio', 'Silva', 'ordenou', 'Polcia', 'imediata', 'reciprocidade', 'quinta-feira', 'americanos', 'comearam', 'ser', 'fotografados', 'identificados', 'digitais', 'entrar', 'territrio', 'brasileiro', 'Yes', 'juzes', 'possvel', 'ordem', 'Silva', 'contestada', 'necessrio', 'governo', 'brasileiro', 'alguma', 'resposta', 'tratamento', 'nacionais', 'recebendo', 'burocracia', 'americana', 'Lula', 'repete', 'razo', 'governo', 'deve', 'defender', 'interesses', 'brasileiros', 'Est', 'diante', 'belo', 'caso', 'Admita-se', 'deciso', 'juiz', 'demasia', 'Ele', 'diretor', 'jurdico', 'Associao', 'Juzes', 'Federais', 'Sua', 'comparao', 'leis', 'Bush', 'piores', 'horrores', 'nazismo', 'horror', 'exagero', 'histrico', 'Mesmo', 'assim', 'campo', 'reciprocidade', 'Depois', 'colecionar', 'abraos', 'ditadores', 'eterno', 'Fidel', 'Castro', 'jovem', 'Assad', 'velho', 'Kadafi', 'Lula', 'oportunidade', 'praticar', 'ato', 'diplomtico', 'ver', 'vida', 'cidados', 'governa', 'governo', 'americano', 'exige', 'brasileiros', 'pretendem', 'Estados', 'Unidos', 'compaream', 'consulados', 'entrevistas', 'pessoais', 'cidado', 'vive', 'Mato', 'Grosso', 'Sul', 'deve', 'vir', 'Paulo', 'cumprir', 'exigncia', 'inteiro', 'direito', 'americanos', 'pedir', 'bem', 'entenderem', 'pois', 'pas', 'vai', 'quer', 'direito', 'brasileiros', 'pedir', 'americanos', 'cumpram', 'exigem', 'Ocorreram', 'casos', 'grotescos', 'humilhao', 'brasileiros', 'entravam', 'devida', 'documentao', 'territrio', 'americano', 'poucos', 'meses', 'mdica', 'ex-presidente', 'Sociedade', 'Mineira', 'Cardiologia', 'deportada', 'viajava', 'reunio', 'internacional', 'Tecnicamente', 'visto', 'errado', 'pois', 'carimbo', 'turista', 'pode', 'ser', 'considerado', 'imprprio', 'vai', 'congresso', 'bem-aventurana', 'posta', 'mesa', 'juiz', 'Julier', 'Sebastio', 'Silva', 'deve', 'ser', 'mantida', 'refinada', 'Coisa', 'assim', 'todos', 'americanos', 'pedirem', 'visto', 'entrar', 'Brasil', 'recebem', 'folheto', 'descrio', 'exigncias', 'governo', 'faz', 'entrada', 'brasileiros', 'Seriam', 'convidados', 'assinar', 'documento', 'pedindo', 'governo', 'Pindorama', 'trate', 'governo', 'americano', 'trata', 'brasileiros', 'Quem', 'pedir', 'leva', 'Quem', 'preferir', 'reciprocidade', 'ganha', 'Tratamento', 'George', 'Bush', 'pode', 'deixar', 'doutor', 'Bush', 'acreditar', 'aqui', 'casa', 'sogra', 'Data', 'Publicao', '01/01/2004', 'Pagina', 'Editoria', 'At2']\n"
     ]
    }
   ],
   "source": [
    "print(documentos_sem_stop_words_e_ignore[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "YGVBOWF1__-F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGVBOWF1__-F",
    "outputId": "02de7d14-026d-42fe-8665-c6c4df2c087d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['casa', 'sogra', 'aqui', 'ELIO', 'GASPARINI', 'Opresidente', 'George', 'Bush', 'est', 'envenenando', 'relao', 'Estados', 'Unidos', 'mundo', 'Seu', 'governo', 'decidiu', 'partir', 'amanh', 'estrangeiros', 'entrarem', 'territrio', 'americano', 'sero', 'fotografados', 'deixaro', 'impresses', 'digitais', 'banco', 'dados', 'procedimento', 'ser', 'feito', 'sujar', 'pessoas', 'Ficaro', 'regra', 'nacionais', 'pases', 'quase', 'todos', 'europeus', 'provvel', 'medida', 'atinja', 'milhes', 'pessoas', 'nao', 'brasileira', 'passou', 'constrangimento', 'saber', '2001', 'ministro', 'Relaes', 'Exteriores', 'Celso', 'Lafer', 'tirar', 'sapatos', 'duas', 'revistas', 'diferentes', 'durante', 'viagens', 'Estados', 'Unidos', 'nico', 'impertinncia', 'atingiu', 'russo', 'chilena', 'Bem-aventurado', 'juiz', 'federal', 'Julier', 'Sebastio', 'Silva', 'ordenou', 'Polcia', 'imediata', 'reciprocidade', 'quinta-feira', 'americanos', 'comearam', 'ser', 'fotografados', 'identificados', 'digitais', 'entrar', 'territrio', 'brasileiro', 'Yes', 'juzes', 'possvel', 'ordem', 'Silva', 'contestada', 'necessrio', 'governo', 'brasileiro', 'alguma', 'resposta', 'tratamento', 'nacionais', 'recebendo', 'burocracia', 'americana', 'Lula', 'repete', 'razo', 'governo', 'deve', 'defender', 'interesses', 'brasileiros', 'Est', 'diante', 'belo', 'caso', 'Admita-se', 'deciso', 'juiz', 'demasia', 'Ele', 'diretor', 'jurdico', 'Associao', 'Juzes', 'Federais', 'Sua', 'comparao', 'leis', 'Bush', 'piores', 'horrores', 'nazismo', 'horror', 'exagero', 'histrico', 'Mesmo', 'assim', 'campo', 'reciprocidade', 'Depois', 'colecionar', 'abraos', 'ditadores', 'eterno', 'Fidel', 'Castro', 'jovem', 'Assad', 'velho', 'Kadafi', 'Lula', 'oportunidade', 'praticar', 'ato', 'diplomtico', 'ver', 'vida', 'cidados', 'governa', 'governo', 'americano', 'exige', 'brasileiros', 'pretendem', 'Estados', 'Unidos', 'compaream', 'consulados', 'entrevistas', 'pessoais', 'cidado', 'vive', 'Mato', 'Grosso', 'Sul', 'deve', 'vir', 'Paulo', 'cumprir', 'exigncia', 'inteiro', 'direito', 'americanos', 'pedir', 'bem', 'entenderem', 'pois', 'pas', 'vai', 'quer', 'direito', 'brasileiros', 'pedir', 'americanos', 'cumpram', 'exigem', 'Ocorreram', 'casos', 'grotescos', 'humilhao', 'brasileiros', 'entravam', 'devida', 'documentao', 'territrio', 'americano', 'poucos', 'meses', 'mdica', 'ex-presidente', 'Sociedade', 'Mineira', 'Cardiologia', 'deportada', 'viajava', 'reunio', 'internacional', 'Tecnicamente', 'visto', 'errado', 'pois', 'carimbo', 'turista', 'pode', 'ser', 'considerado', 'imprprio', 'vai', 'congresso', 'bem-aventurana', 'posta', 'mesa', 'juiz', 'Julier', 'Sebastio', 'Silva', 'deve', 'ser', 'mantida', 'refinada', 'Coisa', 'assim', 'todos', 'americanos', 'pedirem', 'visto', 'entrar', 'Brasil', 'recebem', 'folheto', 'descrio', 'exigncias', 'governo', 'faz', 'entrada', 'brasileiros', 'Seriam', 'convidados', 'assinar', 'documento', 'pedindo', 'governo', 'Pindorama', 'trate', 'governo', 'americano', 'trata', 'brasileiros', 'Quem', 'pedir', 'leva', 'Quem', 'preferir', 'reciprocidade', 'ganha', 'Tratamento', 'George', 'Bush', 'pode', 'deixar', 'doutor', 'Bush', 'acreditar', 'aqui', 'casa', 'sogra', 'Data', 'Publicao', '01/01/2004', 'Pagina', 'Editoria', 'At2'] , 1\n",
      "['Data', 'Publicao', 'Pagina', 'Editoria'] , 1\n"
     ]
    }
   ],
   "source": [
    "rep = 0\n",
    "for k in range(0, len(documentos_sem_stop_words_e_ignore)-1):\n",
    "  if (documentos_sem_stop_words_e_ignore[k]==documentos_sem_stop_words_e_ignore[k+1]):\n",
    "    rep += 1\n",
    "    if (k==len(documentos_sem_stop_words_e_ignore)-2):\n",
    "       print(documentos_sem_stop_words_e_ignore[k],',', rep+1)\n",
    "  else:\n",
    "    print(documentos_sem_stop_words_e_ignore[k],',', rep+1)   \n",
    "    rep=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "HDiR3D4E__-G",
   "metadata": {
    "id": "HDiR3D4E__-G"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text  import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "Sl7Gw6p7__-H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "Sl7Gw6p7__-H",
    "outputId": "b45711d7-9de4-414f-d305-5bdffb2bc45c"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-30061a864839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocumentos_sem_stop_words_e_ignore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1834\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1836\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1837\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(documentos_sem_stop_words_e_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EMHi_5SV__-I",
   "metadata": {
    "id": "EMHi_5SV__-I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "projetoIntegradorClassificaçãoTexto.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
